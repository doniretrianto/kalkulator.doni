{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doniretrianto/kalkulator.doni/blob/master/Web_Scraping_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Install Library**"
      ],
      "metadata": {
        "id": "eD7VyAiltCeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini, kita melakukan instalasi beberapa library yang dibutuhkan."
      ],
      "metadata": {
        "id": "Rlry9PX6uCQq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JR6UYPqjs8EN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7d6d0f-dc65-4097-b315-919330051eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Import Library**"
      ],
      "metadata": {
        "id": "JOksRXPStaE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk menggunakan library pada Python, kita harus melakukan import library tersebut menggunakan perintah `import`."
      ],
      "metadata": {
        "id": "EguBRuV_uIjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time # rate limiting\n",
        "import pandas as pd # untuk export csv"
      ],
      "metadata": {
        "id": "RqQwmxIVtc3m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Fungsi Scrap City (`scrap_city`)**"
      ],
      "metadata": {
        "id": "47qxZt9wtLJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi `scrap_city` berfungsi untuk mengambil data-data rumah pada kota tertentu."
      ],
      "metadata": {
        "id": "UPvuHE24uTGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrap_city(city, url, page=1):\n",
        "    print(f\"Scraping halaman {page} untuk city {city}\")\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "    }\n",
        "\n",
        "    # ambil content HTML pada url tersebut\n",
        "    response = requests.get(url + f\"?page={page}\", headers=headers)\n",
        "\n",
        "    # check response\n",
        "    print(response.status_code)\n",
        "\n",
        "    # print(response.text)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # ambil container utama\n",
        "    container = soup.find('div', {'class': 'card-list-section'})\n",
        "\n",
        "    # ambil semua elemen rumah\n",
        "    featured = container.find_all('div', {'class': 'featured-card-component'})\n",
        "\n",
        "    print(f\"Jumlah rumah: {len(featured)}\")\n",
        "\n",
        "    data = [] # untuk menyimpan data hasil scraping\n",
        "\n",
        "    # loop untuk setiap rumah\n",
        "    for idx, house in enumerate(featured):\n",
        "        print(f\"Scraping rumah ke-{idx + 1}\")\n",
        "\n",
        "        # ambil container untuk rumah\n",
        "        content = house.find('div', {'class': 'card-featured__middle-section'})\n",
        "\n",
        "        if content is None:\n",
        "            continue\n",
        "\n",
        "        price = content.find('div', {'class': 'card-featured__middle-section__price'}).text.strip()\n",
        "        # print(price)\n",
        "\n",
        "        # ambil judul rumah, anak ke-3 dari content\n",
        "        title = content.contents[2].text.strip()\n",
        "\n",
        "        # ambil lokasi rumah, anak ke-4 dari content\n",
        "        location = content.contents[3].text.strip()\n",
        "\n",
        "        #print(title)\n",
        "        #print(location)\n",
        "\n",
        "        features = content.find('div', {'class': 'card-featured__middle-section__attribute'})\n",
        "        attributes = features.find_all('span', {'class': 'attribute-text'})\n",
        "\n",
        "        bedrooms = attributes[0].text.strip()\n",
        "        bathrooms = attributes[1].text.strip()\n",
        "        garage = attributes[2].text.strip() if len(attributes) > 2 else \"\"\n",
        "\n",
        "        area = features.contents[1].text.strip() # anak ke-2\n",
        "        building_area = features.contents[2].text.strip() # anak ke-3\n",
        "\n",
        "        data.append({\n",
        "            'city': city,\n",
        "            'title': title,\n",
        "            'price': price,\n",
        "            'location': location,\n",
        "            'area': area,\n",
        "            'building_area': building_area,\n",
        "            'bedrooms': bedrooms,\n",
        "            'bathrooms': bathrooms,\n",
        "            'garage': garage\n",
        "        })\n",
        "\n",
        "    time.sleep(3) # pause 3 second\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "COUXuu4ntZW8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Fungsi untuk Mulai Scrap**"
      ],
      "metadata": {
        "id": "M5GDFjyotkvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi `start_scrap` berfungsi untuk memulai proses scraping pada beberapa kota. Didalamnya, fungsi ini akan memanggil fungsi `scrap_city`."
      ],
      "metadata": {
        "id": "pfPDYWydukB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def start_scraping(cities, max_page_per_city = 10):\n",
        "    data = []\n",
        "\n",
        "    for city, url in cities.items():\n",
        "        for page in range(1, max_page_per_city + 1):\n",
        "            page_data = scrap_city(city, url, page)\n",
        "            data.extend(page_data)\n",
        "\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "uphebLMtt9kU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Program Utama**"
      ],
      "metadata": {
        "id": "4mUtpdDkt96e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ini adalah program utama. Program akan dimulai pada kode ini."
      ],
      "metadata": {
        "id": "lmT9B9knuypd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cities = {\n",
        "    \"Jakarta Barat\": \"https://www.rumah123.com/jual/jakarta-barat/rumah/\",\n",
        "    \"Jakarta Timur\": \"https://www.rumah123.com/jual/jakarta-timur/rumah/\",\n",
        "    \"Jakarta Utara\": \"https://www.rumah123.com/jual/jakarta-utara/rumah/\",\n",
        "    \"Jakarta Pusat\": \"https://www.rumah123.com/jual/jakarta-pusat/rumah/\",\n",
        "    \"Depok\": \"https://www.rumah123.com/jual/depok/rumah/\",\n",
        "}\n",
        "\n",
        "#scrap_city(\"Jakarta Barat\",\n",
        "#           \"https://www.rumah123.com/jual/jakarta-barat/rumah/\")\n",
        "\n",
        "data = start_scraping(cities, 2)\n",
        "\n",
        "\n",
        "# simpan ke dalam csv\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('harga_rumah.csv', index=False)\n"
      ],
      "metadata": {
        "id": "cAs_3X-tuBH9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b405b5c8-d987-4886-c1cf-2a4972e30869"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping halaman 1 untuk city Jakarta Barat\n",
            "200\n",
            "Jumlah rumah: 20\n",
            "Scraping rumah ke-1\n",
            "Scraping rumah ke-2\n",
            "Scraping rumah ke-3\n",
            "Scraping rumah ke-4\n",
            "Scraping rumah ke-5\n",
            "Scraping rumah ke-6\n",
            "Scraping rumah ke-7\n",
            "Scraping rumah ke-8\n",
            "Scraping rumah ke-9\n",
            "Scraping rumah ke-10\n",
            "Scraping rumah ke-11\n",
            "Scraping rumah ke-12\n",
            "Scraping rumah ke-13\n",
            "Scraping rumah ke-14\n",
            "Scraping rumah ke-15\n",
            "Scraping rumah ke-16\n",
            "Scraping rumah ke-17\n",
            "Scraping rumah ke-18\n",
            "Scraping rumah ke-19\n",
            "Scraping rumah ke-20\n",
            "Scraping halaman 2 untuk city Jakarta Barat\n",
            "200\n",
            "Jumlah rumah: 20\n",
            "Scraping rumah ke-1\n",
            "Scraping rumah ke-2\n",
            "Scraping rumah ke-3\n",
            "Scraping rumah ke-4\n",
            "Scraping rumah ke-5\n",
            "Scraping rumah ke-6\n",
            "Scraping rumah ke-7\n",
            "Scraping rumah ke-8\n",
            "Scraping rumah ke-9\n",
            "Scraping rumah ke-10\n",
            "Scraping rumah ke-11\n",
            "Scraping rumah ke-12\n",
            "Scraping rumah ke-13\n",
            "Scraping rumah ke-14\n",
            "Scraping rumah ke-15\n",
            "Scraping rumah ke-16\n",
            "Scraping rumah ke-17\n",
            "Scraping rumah ke-18\n",
            "Scraping rumah ke-19\n",
            "Scraping rumah ke-20\n",
            "Scraping halaman 1 untuk city Jakarta Timur\n",
            "429\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'find_all'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2176713566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#           \"https://www.rumah123.com/jual/jakarta-barat/rumah/\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_scraping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2864345939.py\u001b[0m in \u001b[0;36mstart_scraping\u001b[0;34m(cities, max_page_per_city)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_page_per_city\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mpage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrap_city\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1766729574.py\u001b[0m in \u001b[0;36mscrap_city\u001b[0;34m(city, url, page)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# ambil semua elemen rumah\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfeatured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'featured-card-component'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Jumlah rumah: {len(featured)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
          ]
        }
      ]
    }
  ]
}